<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2 id="overview">Overview</h2> <p>TreeCLR is my implementation of <strong>supervised contrastive learning</strong> for datasets with <strong>hierarchical multi-label structure</strong>. Instead of treating all non-matching labels as equally negative, this variant uses a tree of class relationships (child–parent mappings) to shape which examples are considered positives and how strongly they are pulled together in embedding space. :contentReference[oaicite:0]{index=0}</p> <p>Concretely, the code adapts the supervised contrastive loss to leverage a label hierarchy (e.g., ImageNet’s WordNet tree) so that semantically close classes produce more similar representations, while still separating distant branches of the tree.</p> <h2 id="what-i-implemented">What I implemented</h2> <ul> <li>A <strong>TreeCLR loss</strong> (see <code class="language-plaintext highlighter-rouge">SupTreeConLoss.py</code>) that extends supervised contrastive loss to hierarchical labels.</li> <li>Data loaders and utilities for <strong>ImageNet</strong> with child→parent label mappings (e.g., <code class="language-plaintext highlighter-rouge">child_to_parent_labels.txt</code>, <code class="language-plaintext highlighter-rouge">imgnet_parent_label.txt</code>). :contentReference[oaicite:1]{index=1}</li> <li>Training scripts with <strong>LARS optimization</strong> and standard contrastive-learning tricks.</li> <li>Evaluation via <strong>k-NN classification</strong> and <strong>linear probing</strong> on frozen embeddings.</li> </ul> <h2 id="highlights">Highlights</h2> <ul> <li>Shows how to plug a <strong>label tree</strong> into a standard supervised contrastive framework.</li> <li>Provides a template for experimenting with <strong>hierarchy-aware representation learning</strong>.</li> <li>Clean PyTorch implementation that can be adapted to other hierarchical datasets beyond ImageNet.</li> </ul> </body></html>