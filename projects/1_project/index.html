<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>TreeCLR is a PyTorch reimplementation of the <em>Use All The Labels: A Hierarchical Multi-Label Contrastive Learning Framework</em> (CVPR 2022). It adapts supervised contrastive learning to hierarchical label spaces, so nearby nodes in the label tree are embedded closer than distant ones.</p> <hr> <h2 id="-summary">üîç Summary</h2> <p>Instead of treating all non-matching classes as equally negative, TreeCLR leverages a label hierarchy (e.g., WordNet for ImageNet) to:</p> <ul> <li>Treat semantically related classes as <strong>softer negatives</strong> or <strong>additional positives</strong>.</li> <li>Encode the label tree into the contrastive loss so that the representation space preserves coarse-to-fine semantics.</li> <li>Improve downstream performance on classification and retrieval under hierarchical labels.</li> </ul> <hr> <h2 id="Ô∏è-what-i-implemented">üõ†Ô∏è What I implemented</h2> <ul> <li>A <strong>hierarchy-aware contrastive loss</strong> extending supervised contrastive learning to multi-label trees.</li> <li>Data loading utilities for ImageNet-style datasets with child‚Üíparent mappings.</li> <li>Training &amp; evaluation scripts for: <ul> <li>Representation learning with TreeCLR.</li> <li>k-NN / linear probing on the learned embeddings.</li> </ul> </li> </ul> <hr> <h2 id="-github-repository">üìÇ GitHub repository</h2> <ul> <li>Code &amp; experiments: <strong><a href="https://github.com/manhbeo/TreeCLR" rel="external nofollow noopener" target="_blank">github.com/manhbeo/TreeCLR</a></strong> </li> <li>Reference paper: <em>Use All The Labels: A Hierarchical Multi-Label Contrastive Learning Framework</em><br> (arXiv:2204.13207).</li> </ul> </body></html>