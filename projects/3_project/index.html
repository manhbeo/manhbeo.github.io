<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2 id="overview">Overview</h2> <p>This repository contains my solution to the <strong>2023 eBay Named Entity Recognition (NER) task</strong> on EvalAI, where the goal is to tag entities in <strong>10,000,000+ product titles</strong>. Because of competition rules, the dataset cannot be shared, but the full modeling pipeline is available in notebooks and scripts. :contentReference[oaicite:4]{index=4}</p> <p>The main challenge is scaling NER models to very large, noisy e-commerce text while staying within memory constraints.</p> <h2 id="approach">Approach</h2> <p>I explored several <strong>sequence labeling</strong> architectures:</p> <ul> <li>An initial setup with <strong>two BiLSTM models</strong>: <ul> <li>One model to detect multi-word tokens.</li> <li>A second model to perform NER on those tokens.</li> </ul> </li> <li>This approach produced an extremely large token list that was difficult to handle on a single machine. :contentReference[oaicite:5]{index=5}</li> </ul> <p>To address this, I switched to a classic <strong>BI tagging scheme</strong>:</p> <ul> <li>For each word at the beginning of an entity span, I prepend <code class="language-plaintext highlighter-rouge">"B-"</code> to its tag.</li> <li>For subsequent words inside the span, I prepend <code class="language-plaintext highlighter-rouge">"I-"</code> to their tags.</li> <li>I then train BiLSTM / BiLSTM-CRF models directly at the <strong>word level</strong>, which is more memory-friendly. :contentReference[oaicite:6]{index=6}</li> </ul> <p>Because the prediction set is very large, inference is parallelized to keep runtime manageable. :contentReference[oaicite:7]{index=7}</p> <h2 id="highlights">Highlights</h2> <ul> <li>Practical experience with <strong>industrial-scale NER</strong> (10M titles).</li> <li>Comparison of <strong>token-level vs. word-level</strong> tagging schemes under tight memory constraints.</li> <li>End-to-end pipeline mixing <strong>Python (modeling)</strong> and <strong>R (data cleaning / preprocessing)</strong> for a real-world competition environment.</li> </ul> </body></html>